{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5835346",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a335609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AddedToken, GPT2Config, GPT2Tokenizer, GPT2ForSequenceClassification, GPT2Model\n",
    "from text_utils import custom_data_gen, DBpedia, TextProcessor, TextTokenizer\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils import META_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc0bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-5\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 100\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e25ef1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({\n",
    "    \"pad_token\": AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99425e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to do\n",
    "## re split: part of train use for quantization, part of test use for examples\n",
    "\n",
    "## just change contents of csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27212799",
   "metadata": {},
   "source": [
    "__Plan__\n",
    "\n",
    "__1.1. re split data 19:05 DONE__\n",
    "  \n",
    "__1.2. new dataloaders 19:20 DONE__\n",
    "\n",
    "__2. write train loop 19:40__\n",
    "\n",
    "__3. train gpt2 several attention layers 20:00__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f5dc14",
   "metadata": {},
   "source": [
    "__1. re split data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7862fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = DBpedia(root=\"/root/quantized_nn/dbpedia_data\")\n",
    "\n",
    "# train_data = pd.read_csv(\"dbpedia_data/dbpedia_csv/train.csv\", header=None)\n",
    "# test_data = pd.read_csv(\"dbpedia_data/dbpedia_csv/test.csv\", header=None)\n",
    "\n",
    "# train_data.columns = [\"class\", \"header\", \"content\"]\n",
    "# test_data.columns = [\"class\", \"header\", \"content\"]\n",
    "\n",
    "\n",
    "\n",
    "# train_data, quant_data = train_test_split(train_data,\n",
    "#                                           test_size=0.2,\n",
    "#                                           stratify=train_data[\"class\"],\n",
    "#                                           random_state=1234567890)\n",
    "# test_data, infer_data = train_test_split(test_data,\n",
    "#                                          test_size=0.5,\n",
    "#                                          stratify=test_data[\"class\"],\n",
    "#                                          random_state=1234567890)\n",
    "\n",
    "# os.mkdir(\"data_db/\")\n",
    "# train_data.to_csv(\"data_db/train.csv\", index=None)\n",
    "# quant_data.to_csv(\"data_db/quant.csv\", index=None)\n",
    "# test_data.to_csv(\"data_db/test.csv\", index=None)\n",
    "# infer_data.to_csv(\"data_db/infer.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbee6e0",
   "metadata": {},
   "source": [
    "__2. NEW DATALOADERS__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"data_db/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a185a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data_db/train.csv\")\n",
    "quant_data = pd.read_csv(\"data_db/quant.csv\")\n",
    "test_data = pd.read_csv(\"data_db/test.csv\")\n",
    "infer_data = pd.read_csv(\"data_db/infer.csv\")\n",
    "tst_0_data = train_data.iloc[:320]\n",
    "tst_1_data = train_data.iloc[320:2*320]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8abb180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = ((row[0], row[2]) for row in train_data.values)\n",
    "data_quant = ((row[0], row[2]) for row in quant_data.values)\n",
    "data_test = ((row[0], row[2]) for row in test_data.values)\n",
    "data_infer = ((row[0], row[2]) for row in infer_data.values)\n",
    "data_tst_0 = ((row[0], row[2]) for row in tst_0_data.values)\n",
    "data_tst_1 = ((row[0], row[2]) for row in tst_1_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e43d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "\n",
    "for text, lab in quant_loader:\n",
    "    print(text.size())\n",
    "    print(lab.size())\n",
    "    k += 1\n",
    "    if k == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_train = {i: 0 for i in range(1, 15)}\n",
    "labs_test = {i: 0 for i in range(1, 15)}\n",
    "\n",
    "for lab, _ in train_data:\n",
    "    labs_train[lab] += 1\n",
    "    \n",
    "for lab, _ in test_data:\n",
    "    labs_test[lab] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b2104c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "gpt2_config = GPT2Config(vocab_size=len(tokenizer.get_vocab()))\n",
    "setattr(gpt2_config, \"pad_token_id\", 50256)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"gpt2\",\n",
    "                                                      config=gpt2_config)\n",
    "\n",
    "new_score = nn.Sequential(\n",
    "    nn.Linear(in_features=768, out_features=14, bias=False),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "setattr(model, \"score\", new_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e25c8e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"gpt2_dbpedia_2305.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca8af17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 149 layers\n"
     ]
    }
   ],
   "source": [
    "num_layers = sum(1 for p in model.parameters())\n",
    "print(f\"model has {num_layers} layers\")\n",
    "\n",
    "trainable_num = 12\n",
    "\n",
    "for num, (nam, p) in enumerate(model.named_parameters()):\n",
    "    if num > num_layers - trainable_num:\n",
    "        p.requires_grad = True\n",
    "    else:\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8018804",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9933a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, lab in train_loader:\n",
    "    print(torch.argmax(model(text).logits, dim=-1))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ecfa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tst_0 = ((row[2], row[0]) for row in tst_0_data.values)\n",
    "data_tst_1 = ((row[2], row[0]) for row in tst_1_data.values)\n",
    "\n",
    "test_ldr_0 = custom_data_gen(data_tst_0,\n",
    "                               tokenizer,\n",
    "                               MAX_LEN,\n",
    "                               BATCH_SIZE,\n",
    "                               DEVICE)\n",
    "\n",
    "test_ldr_1 = custom_data_gen(data_tst_1,\n",
    "                               tokenizer,\n",
    "                               MAX_LEN,\n",
    "                               BATCH_SIZE,\n",
    "                               DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "156d540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 train_data,\n",
    "                 test_data,\n",
    "                 batch_size,\n",
    "                 n_epochs,\n",
    "                 plateau,\n",
    "                 lr,\n",
    "                 lr_decay,\n",
    "                 device,\n",
    "                 res_path,\n",
    "                 loss_fn=None,\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.plateau = plateau\n",
    "        self.lr = lr\n",
    "        self.lr_decay = lr_decay\n",
    "        self.device = device\n",
    "        self.res_path = res_path\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), self.lr)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        if loss_fn:\n",
    "            self.loss_fn = getattr(nn, loss_fn)()\n",
    "        self.history = {}\n",
    "    \n",
    "    def _dump_history(self):\n",
    "        with open(self.res_path, \"w\") as f:\n",
    "            json.dump(self.history, f)\n",
    "    \n",
    "    def _update_optimizer(self):\n",
    "        print(f\"LR reduced on plateau from {self.lr} to {self.lr*self.lr_decay}\")\n",
    "        self.lr *= self.lr_decay\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), self.lr)\n",
    "    \n",
    "    def train(self):\n",
    "        self.history = {\n",
    "            \"loss\": [],\n",
    "            \"acc\": [],\n",
    "            \"best_loss\": 1e10\n",
    "        }\n",
    "        no_gain = 0\n",
    "        for epoch in range(1, self.n_epochs+1):\n",
    "            loss, acc = self.train_epoch(epoch)\n",
    "            if float(loss) < self.history[\"best_loss\"]:\n",
    "                self.history[\"best_loss\"] = float(loss)\n",
    "                no_gain = 0\n",
    "            else:\n",
    "                no_gain += 1\n",
    "            if no_gain == self.plateau:\n",
    "                self._update_optimizer()\n",
    "            self.history[\"loss\"].append(float(loss))\n",
    "            self.history[\"acc\"].append(float(acc))\n",
    "            self._dump_history()\n",
    "        return self.model\n",
    "            \n",
    "    def train_epoch(self,\n",
    "                    epoch):\n",
    "        loss_trn = 0\n",
    "        batch_trn = 0\n",
    "        loss_val = 0\n",
    "        acc_val = 0\n",
    "        batch_val = 0\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        data_tst_0 = ((row[0], row[2]) for row in self.train_data.values)\n",
    "        data_tst_1 = ((row[0], row[2]) for row in self.test_data.values)\n",
    "        for x, y in tqdm(custom_data_gen(data_tst_0,\n",
    "                                         tokenizer,\n",
    "                                         MAX_LEN,\n",
    "                                         self.batch_size,\n",
    "                                         self.device)):\n",
    "            loss = self.run_batch(x,\n",
    "                                  y)\n",
    "            batch_trn += 1\n",
    "            loss_trn += loss.item()\n",
    "        \n",
    "        for x, y in tqdm(custom_data_gen(data_tst_1,\n",
    "                                         tokenizer,\n",
    "                                         MAX_LEN,\n",
    "                                         self.batch_size,\n",
    "                                         self.device)):\n",
    "            loss, acc = self.run_batch(x,\n",
    "                                       y,\n",
    "                                       train=False)\n",
    "            batch_val += 1\n",
    "            loss_val += loss.item()\n",
    "            acc_val += acc.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch}: val.loss - {loss_val / batch_val}, val.acc - {acc_val / batch_val}\")\n",
    "        return loss_val / batch_val, acc_val / batch_val\n",
    "    \n",
    "    def run_batch(self,\n",
    "                  x,\n",
    "                  y,\n",
    "                  train=True):\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        preds = self.model(x).logits\n",
    "        loss = self.loss_fn(preds, y.long())\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            return loss\n",
    "        else:\n",
    "            acc = torch.sum(torch.argmax(preds, dim=-1) == y) / self.batch_size\n",
    "            return loss, acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff7deaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model,\n",
    "                  train_data,\n",
    "                  test_data,\n",
    "                  BATCH_SIZE,\n",
    "                  n_epochs=20,\n",
    "                  plateau=3,\n",
    "                  lr=LR,\n",
    "                  lr_decay=0.8,\n",
    "                  device=DEVICE,\n",
    "                  res_path=\"results_gpt2_2405_morning.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01546bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7ede639bd344f58f57fdd577a1c41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-651413d48952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-c66717367a24>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mno_gain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"best_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"best_loss\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-c66717367a24>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     78\u001b[0m                                          self.device)):\n\u001b[1;32m     79\u001b[0m             loss = self.run_batch(x,\n\u001b[0;32m---> 80\u001b[0;31m                                   y)\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mbatch_trn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mloss_trn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-c66717367a24>\u001b[0m in \u001b[0;36mrun_batch\u001b[0;34m(self, x, y, train)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1300\u001b[0m                 )\n\u001b[1;32m   1301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m         \u001b[0mpooled_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_lengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_trained = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3098edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09afd342",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_trained.state_dict(), \"gpt2_dbpedia_2305.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add lr reduction on plateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07787c81",
   "metadata": {},
   "source": [
    "__transformers search__\n",
    "\n",
    "1. models/gpt2/modeling_gpt2.py class GPT2ForSequenceClassification(GPT2PreTrainedModel)\n",
    "2. models/gpt2/modeling_gpt2.py class GPT2PreTrainedModel(PreTrainedModel)\n",
    "3. modeling_utils.py class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin)\n",
    "4. modeling_utils.py class ModuleUtilsMixin\n",
    "5. generation_utils.py class GenerationMixin\n",
    "6. file_utils.py class PushToHubMixin\n",
    "7. models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel) (maybe embedding quantization can be ussed)\n",
    "8. models/gpt2/modeling_gpt2.py class GPT2Block(nn.Module)\n",
    "9. models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module) (__modify this to quantize attention block__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2d95cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0255ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = sum(torch.mul(*p.size()) for p in model.parameters() if len(p.size()) > 1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5280af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2920972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "768*768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mul(*torch.Tensor([768, 768]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = p.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mul(*x).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b8d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc26a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51c2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8431b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc47c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
